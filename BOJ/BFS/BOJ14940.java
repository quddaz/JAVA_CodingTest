트러블슈팅
| 공공데이터 CSV 파일 갱신 시 지연 문제
URL : 
공공데이터 CSV 파일 갱신
문제 상황
공공데이터 CSV 파일이 주기적으로 갱신되며, 이 과정에서 최대 10만 건의 데이터 변경 및 삭제 작업이 발생했습니다. 기존 방식에서는 전체 데이터를 갱신하는 데 최대 3분이 소요되었으며, 이로 인해 서비스 운영에 지연이 발생하고 시스템 부하가 증가하는 문제가 있었습니다. 특히, 대량의 변경 작업으로 인해 데이터 정합성 유지와 성능 최적화가 필요했습니다.
개선 쿼리
문제 인식
개선 전, 데이터 갱신 작업에서는 JPA의 saveAll()과 deleteAll()을 사용하여 데이터를 한 번에 추가하거나 삭제하였습니다.
Spring JPA는 대량의 데이터를 효율적으로 처리하기 위해 saveAll()과 deleteAll()을 제공하지만, 공공데이터 CSV 파일을 갱신하는 과정에서 성능 문제가 발생하였습니다.

문제의 원인
saveAll()과 deleteAll()은 내부적으로 JPA의 영속성 컨텍스트를 관리하기 위해 개별 엔티티를 하나씩 처리합니다.
이로 인해 한 번의 메서드 호출에도 불필요한 DB I/O가 증가, 성능 저하를 초래하였습니다.
특히 서버 시작 초기에 실행되는 대량 데이터 갱신 작업에서는 비효율적인 쿼리 실행이 누적되어 문제를 야기하였습니다.

즉 영속성 컨텍스트를 거치지 않고 대용량 처리 방법이 필요했습니다.
개선 방안
해당 문제를 해결할 수 있는 대용량 처리 기술은 여러 가지가 있지만, 저는 JDBC Batch를 선택하였습니다.
데이터 증가 가능성: 해당 공공데이터 파일은 병원 정보로 구성되어 있으며, 단기간 내 급격한 데이터 증가 가능성이 낮다고 판단하였습니다.
기존 기술 스택 활용: Spring Batch와 같은 외부 라이브러리를 도입하는 대신, 현재 프로젝트에서 즉시 활용할 수 있는 JDBC Batch를 사용하는 것이 학습 비용 측면에서 유리하다고 생각했습니다.
결과
CSV 파일 갱신 작업 시간이 약 19.2% 단축되었으며, 대량 데이터 변경 시 성능이 크게 향상되었습니다.